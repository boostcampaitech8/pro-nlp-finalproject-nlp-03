# MSA êµ¬ì¡° ì‹¤í–‰ ê°€ì´ë“œ

## ğŸ—ï¸ ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Main Application (venv)            â”‚
â”‚  - CookingSession                   â”‚
â”‚  - port: N/A                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“ HTTP    â†“ HTTP    â†“ HTTP
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚STT Serverâ”‚ â”‚LLM Serverâ”‚ â”‚TTS Serverâ”‚
â”‚(Whisper) â”‚ â”‚ (Qwen)   â”‚ â”‚(GPT-So)  â”‚
â”‚  8011    â”‚ â”‚  8013    â”‚ â”‚  8012    â”‚
â”‚  venv    â”‚ â”‚venv_llm  â”‚ â”‚  venv    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“¦ ì„¤ì¹˜

### 1. venv í™˜ê²½ (STT + TTS + ë©”ì¸ ì•±)

```bash
cd voice_module

# ì´ë¯¸ ìˆìœ¼ë©´ ì¬ì‚¬ìš©
source venv/bin/activate

# í•„ìš”ì‹œ íŒ¨í‚¤ì§€ í™•ì¸
pip list | grep -E "torch|torchaudio|transformers|fastapi"
```

### 2. venv_llm í™˜ê²½ (LLM ì„œë²„ ì „ìš©) â­ ì‹ ê·œ

```bash
cd voice_module

# ê°€ìƒí™˜ê²½ ìƒì„±
python -m venv venv_llm
source venv_llm/bin/activate

# íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install torch torchaudio --index-url https://download.pytorch.org/whl/cu121
pip install git+https://github.com/huggingface/transformers.git
pip install git+https://github.com/huggingface/peft.git
pip install fastapi uvicorn[standard] accelerate tiktoken einops
pip install pydantic pyyaml

# ë˜ëŠ” requirements ì‚¬ìš©
pip install -r venv_llm_requirements.txt

# Qwen ëª¨ë¸ ë¯¸ë¦¬ ë‹¤ìš´ë¡œë“œ (ì„ íƒì , ê¶Œì¥)
python << 'EOF'
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

model_name = "Qwen/Qwen3-4B-Instruct-2507"
print("Tokenizer ë‹¤ìš´ë¡œë“œ...")
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, use_fast=False)
print("ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (~8GB)...")
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map="auto", trust_remote_code=True)
print("ì™„ë£Œ!")
EOF
```

---

## ğŸš€ ì‹¤í–‰ ìˆœì„œ

### âœ… í•„ìˆ˜: 3ê°œ ì„œë²„ ëª¨ë‘ ì‹¤í–‰ í•„ìš”

### í„°ë¯¸ë„ 0: vLLM ì„œë²„ (5000ë²ˆ) â­ í•„ìˆ˜

```bash
vllm serve jjjunho/Qwen3-4B-Instruct-2507-Korean-AWQ --port 5000 --gpu-memory-utilization 0.6
```

### í„°ë¯¸ë„ 1: LLM ì„œë²„ (8013ë²ˆ, venv_llm) â­

```bash
cd voice_module
source venv_llm/bin/activate

python servers/llm_server.py
```

**ì¶œë ¥ ì˜ˆì‹œ:**
```
[INFO] Qwen LLM ì„œë²„ ì‹œì‘
[INFO] ëª¨ë¸: Qwen/Qwen3-4B-Instruct-2507
[INFO] ë””ë°”ì´ìŠ¤: cuda
[INFO] Qwen ëª¨ë¸ ë¡œë”© ì¤‘...
[INFO] ëª¨ë¸ ë¡œë”© ì™„ë£Œ! (ì†Œìš” ì‹œê°„: 45.23ì´ˆ)
[INFO] ì„œë²„ ì¤€ë¹„ ì™„ë£Œ!
INFO:     Uvicorn running on http://0.0.0.0:8013
```

**í™•ì¸:**
```bash
curl http://localhost:8013/health
# {"status":"healthy","model_loaded":true, ...}
```

---

### í„°ë¯¸ë„ 2: STT ì„œë²„ (8011ë²ˆ, venv)

```bash
cd voice_module
source venv/bin/activate

python servers/stt_server.py
```

**ì¶œë ¥ ì˜ˆì‹œ:**
```
[INFO] Whisper STT ì„œë²„ ì‹œì‘
[INFO] Whisper ëª¨ë¸ ë¡œë”© ì¤‘...
[INFO] Whisper ëª¨ë¸ ë¡œë”© ì™„ë£Œ! (ì†Œìš” ì‹œê°„: 15.32ì´ˆ)
INFO:     Uvicorn running on http://0.0.0.0:8011
```

**í™•ì¸:**
```bash
curl http://localhost:8011/health
# {"status":"healthy","model_loaded":true, ...}
```

---

### í„°ë¯¸ë„ 3: TTS ì„œë²„ (8012ë²ˆ, venv)

```bash
cd voice_module
source venv/bin/activate

python servers/tts_server.py
```

**í™•ì¸:**
```bash
curl http://localhost:8012/health
# {"status":"healthy","model_loaded":true, ...}
```

---

### í„°ë¯¸ë„ 4: ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ (venv)

**3ê°œ ì„œë²„ê°€ ëª¨ë‘ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸ í›„:**

```bash
cd voice_module
source venv/bin/activate

python main.py
```

**ì¶œë ¥ ì˜ˆì‹œ:**
```
[INFO] STT í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
[INFO] TTS í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
[INFO] LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (8013ë²ˆ í¬íŠ¸ ì„œë²„ ì‚¬ìš©)
[INFO] CookingSession ì´ˆê¸°í™” ì™„ë£Œ

í…ŒìŠ¤íŠ¸ ì„ íƒ:
1. í…ìŠ¤íŠ¸ ëª¨ë“œ (ì±—ë´‡)
...
```

---

## ğŸ§ª í…ŒìŠ¤íŠ¸

### 1. ì„œë²„ í—¬ìŠ¤ì²´í¬

```bash
# ëª¨ë“  ì„œë²„ í™•ì¸
curl http://localhost:8011/health  # STT
curl http://localhost:8013/health  # LLM â­
curl http://localhost:8012/health  # TTS
```

### 2. LLM ì„œë²„ ì§ì ‘ í…ŒìŠ¤íŠ¸

```bash
curl -X POST http://localhost:8013/classify \
  -H "Content-Type: application/json" \
  -d '{"text": "ë‹¤ìŒ ë‹¨ê³„ë¡œ ë„˜ì–´ê°€ì¤˜", "current_step": "ì–‘íŒŒë¥¼ ë³¶ì•„ì£¼ì„¸ìš”"}'

# ì˜ˆìƒ ì‘ë‹µ:
# {
#   "success": true,
#   "intent": "next_step",
#   "response": "",
#   "duration_ms": 234.56
# }
```

### 3. ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸

```bash
python main.py
# 1ë²ˆ ì„ íƒ (í…ìŠ¤íŠ¸ ëª¨ë“œ)
# ì…ë ¥: "ë‹¤ìŒ"
# ì¶œë ¥: "2ë‹¨ê³„: ..."
```

---

## ğŸ“Š ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

| ì‘ì—… | ì†Œìš” ì‹œê°„ |
|------|----------|
| LLM ì„œë²„ ì‹œì‘ (ì²« ì‹¤í–‰) | 60ì´ˆ (ëª¨ë¸ ë‹¤ìš´ë¡œë“œ í¬í•¨) |
| LLM ì„œë²„ ì‹œì‘ (ìºì‹œ ì‚¬ìš©) | 30ì´ˆ |
| STT ì„œë²„ ì‹œì‘ | 15ì´ˆ |
| TTS ì„œë²„ ì‹œì‘ | 20ì´ˆ |
| **ì´ ì„œë²„ ì‹œì‘ ì‹œê°„** | **65ì´ˆ** (ë³‘ë ¬ ì‹¤í–‰ ì‹œ) |
| | |
| `session = CookingSession()` | **0.5ì´ˆ** âš¡ |
| `handle_text("ë‹¤ìŒ")` | 1.2ì´ˆ (LLM API í˜¸ì¶œ í¬í•¨) |
| E2E (ìŒì„± â†’ ì‘ë‹µ) | 5ì´ˆ |

---

## ğŸ”§ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ë¬¸ì œ 1: LLM ì„œë²„ ì—°ê²° ì‹¤íŒ¨
```
RuntimeError: LLM ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤
```

**í•´ê²°:**
```bash
# LLM ì„œë²„ ì‹¤í–‰ í™•ì¸
curl http://localhost:8013/health

# ì•ˆ ë˜ë©´ ì¬ì‹œì‘ (venv_llm í™˜ê²½!)
source venv_llm/bin/activate
python servers/llm_server.py
```

---

### ë¬¸ì œ 2: venv_llmì—ì„œ transformers ë²„ì „ í™•ì¸
```bash
source venv_llm/bin/activate
pip show transformers

# ì¶œë ¥: Version: 5.0.1.dev0 (ë˜ëŠ” ìµœì‹ )
```

---

### ë¬¸ì œ 3: GPU ë©”ëª¨ë¦¬ ë¶€ì¡±
```
CUDA out of memory
```

**í•´ê²°:**
```bash
# ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ í™•ì¸
nvidia-smi

# Qwen ëª¨ë¸ë§Œ ì‹¤í–‰ (STT/TTS ì¢…ë£Œ)
# ë˜ëŠ” CPU ëª¨ë“œë¡œ ë³€ê²½ (servers/llm_server.py)
# DEVICE = "cpu"
```

---

### ë¬¸ì œ 4: í¬íŠ¸ ì¶©ëŒ
```
Address already in use: 8013
```

**í•´ê²°:**
```bash
# í”„ë¡œì„¸ìŠ¤ í™•ì¸
lsof -i :8013

# ì¢…ë£Œ
kill -9 <PID>

# ë˜ëŠ” ë‹¤ë¥¸ í¬íŠ¸ ì‚¬ìš©
# config/settings.py: LLM_SERVER_URL = "http://localhost:8004"
# servers/llm_server.py: port=8004
```

---

## ğŸ’¡ ì¥ì 

### âœ… ì˜ì¡´ì„± ì¶©ëŒ í•´ê²°
- venv: transformers 4.43~4.50 (GPT-SoVITS)
- venv_llm: transformers ìµœì‹  (Qwen)
- ì™„ì „ ë¶„ë¦¬!

### âœ… ë…ë¦½ì  ë°°í¬
- LLM ì„œë²„ë§Œ ì¬ì‹œì‘ ê°€ëŠ¥
- ë©”ì¸ ì•± ì¤‘ë‹¨ ì—†ìŒ

### âœ… ë¹ ë¥¸ ì‹œì‘
- `CookingSession()` ìƒì„±: 0.5ì´ˆ
- LLM ë¡œë”©ì€ ì„œë²„ì—ì„œ ë¯¸ë¦¬ ì™„ë£Œ

### âœ… ìŠ¤ì¼€ì¼ë§
- LLM ì„œë²„ë¥¼ ë” í° GPU ì¸ìŠ¤í„´ìŠ¤ë¡œ ë¶„ë¦¬
- STT/TTSëŠ” CPU ì¸ìŠ¤í„´ìŠ¤

---

## ğŸ“ ë³€ê²½ëœ íŒŒì¼

### ì‹ ê·œ íŒŒì¼
- `servers/llm_server.py` - Qwen LLM FastAPI ì„œë²„ (venv_llm)
- `core/types.py` - Intent Enum ê³µí†µ íƒ€ì…
- `venv_llm_requirements.txt` - LLM ì„œë²„ ì˜ì¡´ì„±
- `MSA_GUIDE.md` - ì´ íŒŒì¼

### ìˆ˜ì •ëœ íŒŒì¼
- `core/api_client.py` - LLMClient ì¶”ê°€
- `config/settings.py` - LLM_SERVER_URL ì¶”ê°€
- `agents/cooking_session.py` - LLM ì„œë²„ ì‚¬ìš©ìœ¼ë¡œ ë³€ê²½

### ë°±ì—… íŒŒì¼
- `agents/cooking_session_lazy_backup.py` - ì´ì „ ë²„ì „ (Lazy Loading)
- `agents/cooking_session_eager.py.backup` - ë” ì´ì „ ë²„ì „ (Eager Loading)

---

## ğŸ¯ ë‹¤ìŒ ë‹¨ê³„

### Production ë°°í¬ ì‹œ
1. **Docker Compose ì‚¬ìš©**
   ```yaml
   services:
     llm:
       build: ./llm_server
       ports: ["8013:8013"]
       deploy:
         resources:
           reservations:
             devices:
               - driver: nvidia
                 count: 1
     stt:
       build: ./stt_server
       ports: ["8011:8011"]
     tts:
       ...
   ```

2. **ë¡œë“œ ë°¸ëŸ°ì„œ**
   - Nginxë¡œ LLM ì„œë²„ ì—¬ëŸ¬ ê°œ ë¡œë“œ ë°¸ëŸ°ì‹±

3. **ëª¨ë‹ˆí„°ë§**
   - Prometheus + Grafana
   - ê° ì„œë²„ë³„ ë©”íŠ¸ë¦­ ìˆ˜ì§‘

---

## ğŸ“ ë¬¸ì˜

ë¬¸ì œ ë°œìƒ ì‹œ:
1. ê° ì„œë²„ ë¡œê·¸ í™•ì¸
2. health check ì—”ë“œí¬ì¸íŠ¸ í™•ì¸
3. GPU ë©”ëª¨ë¦¬ í™•ì¸ (`nvidia-smi`)
