# venv_llm 전용 requirements
# Qwen LLM 서버용 (8013번 포트)

# PyTorch (CUDA 12.1)
--extra-index-url https://download.pytorch.org/whl/cu121
torch
torchaudio

# Transformers (최신 버전)
transformers>=4.45.0,<5.0.0
git+https://github.com/huggingface/peft.git

# FastAPI 서버
fastapi
uvicorn[standard]

# LLM 추론 최적화
accelerate
tiktoken
einops

# 추가 필수 패키지
pydantic
pyyaml

# 아래와 같이 다운 받아둠
# pip install torch torchaudio --index-url https://download.pytorch.org/whl/cu121
# pip install git+https://github.com/huggingface/transformers.git
# pip install git+https://github.com/huggingface/peft.git
# pip install fastapi uvicorn accelerate tiktoken einops
# pip install pydantic pyyaml 

# vllm
vllm==0.15.0