"""
Qwen LLM FastAPI ì„œë²„ (venv_llm í™˜ê²½ ì „ìš©)

ì‹¤í–‰ ë°©ë²•:
    source venv_llm/bin/activate
    python servers/llm_server.py

    ë˜ëŠ”:
    uvicorn servers.llm_server:app --host 0.0.0.0 --port 8013

API ì—”ë“œí¬ì¸íŠ¸:
    POST /classify - ì¸í…íŠ¸ ë¶„ë¥˜ ë° ì‘ë‹µ ìƒì„±
    GET /health - ì„œë²„ ìƒíƒœ í™•ì¸
"""

import os
import sys
import re
import json
import time
import logging
from collections import defaultdict
from contextlib import asynccontextmanager
from typing import Optional, Dict, Any

from openai import OpenAI
from dotenv import load_dotenv  # ì¶”ê°€: í™˜ê²½ë³€ìˆ˜ ë¡œë“œ

import torch
import yaml
from fastapi import FastAPI, HTTPException, Security, Depends  # ì¶”ê°€: ë³´ì•ˆ ëª¨ë“ˆ
from fastapi.middleware.cors import CORSMiddleware
from fastapi.security.api_key import APIKeyHeader  # ì¶”ê°€: API Key í—¤ë”
from starlette.status import HTTP_403_FORBIDDEN    # ì¶”ê°€: 403 ìƒíƒœì½”ë“œ
from pydantic import BaseModel, Field
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S"
)
logger = logging.getLogger(__name__)

# ============================================================================
# ì„¤ì •
# ============================================================================

MODEL_NAME = "Qwen/Qwen3-4B-Instruct-2507"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

VLLM_ENDPOINT = "http://localhost:5000/v1"

PROMPTS_YAML_PATH = "./config/prompts.yaml"
USE_VLLM = True  # True: vLLM ì‚¬ìš©, False: transformers ì‚¬ìš©

# Intent ë§¤í•‘
INTENT_MAP = {
    "Next": "next_step",
    "Prev": "prev_step",
    "Finish": "finish",
    "Missing Ingredient": "substitute_ingredient",
    "Missing Tool": "substitute_tool",
    "Failure": "failure",
    "Out of Scope": "unknown",
}

# ============================================================================
# ğŸ”’ ë³´ì•ˆ ì„¤ì • (API Key)
# ============================================================================

# í™˜ê²½ë³€ìˆ˜ì—ì„œ í‚¤ ê°€ì ¸ì˜¤ê¸° (RECIPEU_API_KEY ì‚¬ìš©)
API_KEY = os.environ.get("RECIPEU_API_KEY")

# [ì•ˆì „ì¥ì¹˜] í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìœ¼ë©´ ë¡œê·¸ ê²½ê³ 
if not API_KEY:
    logger.error("âŒ ì¹˜ëª…ì  ì˜¤ë¥˜: 'RECIPEU_API_KEY' í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
    # ì‹¤ì „ ë°°í¬ ì‹œ ì•„ë˜ ì£¼ì„ í•´ì œ ê¶Œì¥
    # raise ValueError("RECIPEU_API_KEY í™˜ê²½ë³€ìˆ˜ ë¯¸ì„¤ì •")

API_KEY_NAME = "X-API-Key"
api_key_header = APIKeyHeader(name=API_KEY_NAME, auto_error=False)

async def get_api_key(api_key_header: str = Security(api_key_header)):
    """API Key ê²€ì¦ í•¨ìˆ˜"""
    if api_key_header == API_KEY:
        return api_key_header
    else:
        raise HTTPException(
            status_code=HTTP_403_FORBIDDEN, 
            detail="ì¸ì¦ ì‹¤íŒ¨: ìœ íš¨í•˜ì§€ ì•Šì€ API Keyì…ë‹ˆë‹¤."
        )

# ============================================================================
# Pydantic ëª¨ë¸
# ============================================================================

class ClassifyRequest(BaseModel):
    """ì¸í…íŠ¸ ë¶„ë¥˜ ìš”ì²­"""
    text: str = Field(..., description="ì‚¬ìš©ì ë°œí™”", min_length=1)
    current_step: str = Field(default="", description="í˜„ì¬ ìš”ë¦¬ ë‹¨ê³„")
    current_cook: str = Field(default="", description="í˜„ì¬ ìš”ë¦¬ ì œëª©")
    recipe_context: str = Field(default="", description="ì¸ì ‘ ë‹¨ê³„ ì •ë³´")
    history: list = Field(default=[], description="ëŒ€í™” ê¸°ë¡ [{role, content}, ...]")

class ClassifyResponse(BaseModel):
    """ì¸í…íŠ¸ ë¶„ë¥˜ ì‘ë‹µ"""
    success: bool
    intent: str
    response: str
    raw_output: Optional[str] = None
    duration_ms: Optional[float] = None

class HealthResponse(BaseModel):
    """ì„œë²„ ìƒíƒœ ì‘ë‹µ"""
    status: str
    model_loaded: bool
    model_name: str
    device: str
    uptime_seconds: float

# ============================================================================
# ì „ì—­ ë³€ìˆ˜
# ============================================================================

llm_pipe: Optional[pipeline] = None
tokenizer: Optional[AutoTokenizer] = None

# vLLM ì „ìš© ë³€ìˆ˜
client: Optional[OpenAI] = None
prompts: Optional[dict] = None
server_start_time: float = 0

# ============================================================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ============================================================================

def load_prompts():
    """í”„ë¡¬í”„íŠ¸ YAML ë¡œë“œ"""
    global prompts
    logger.info(f"í”„ë¡¬í”„íŠ¸ ë¡œë“œ: {PROMPTS_YAML_PATH}")
    try:
        with open(PROMPTS_YAML_PATH, 'r', encoding='utf-8') as f:
            prompts = yaml.safe_load(f)
        logger.info("í”„ë¡¬í”„íŠ¸ ë¡œë“œ ì™„ë£Œ")
    except Exception as e:
        logger.error(f"í”„ë¡¬í”„íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise

def get_prompt(key: str, **kwargs) -> str:
    """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ê°€ì ¸ì˜¤ê¸°"""
    template = prompts.get(key, {}).get('template', "")
    # ëˆ„ë½ëœ í‚¤ëŠ” ë¹ˆ ë¬¸ìì—´ë¡œ ì²˜ë¦¬
    return template.format_map(defaultdict(str, kwargs))

def extract_json(text: str) -> Dict[str, Any]:
    """LLM ì¶œë ¥ì—ì„œ JSON ì¶”ì¶œ"""
    if not text:
        return {}

    match = re.search(r'\{[\s\S]*\}', text)
    if not match:
        logger.warning(f"JSONì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {text[:100]}")
        return {}

    try:
        return json.loads(match.group(0))
    except json.JSONDecodeError as e:
        logger.error(f"JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
        return {}

# ============================================================================
# ì„œë²„ ì‹œì‘/ì¢…ë£Œ ì´ë²¤íŠ¸
# ============================================================================

@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì„œë²„ ì‹œì‘ ì‹œ ëª¨ë¸ ë¡œë“œ"""
    global llm_pipe, tokenizer, client, server_start_time

    logger.info("=" * 60)
    logger.info("Qwen LLM ì„œë²„ ì‹œì‘")
    logger.info("=" * 60)
    logger.info(f"ëª¨ë¸: {MODEL_NAME}")
    logger.info(f"ë””ë°”ì´ìŠ¤: {DEVICE}")
    logger.info(f"vLLM ì‚¬ìš©: {USE_VLLM}")

    # í”„ë¡¬í”„íŠ¸ ë¡œë“œ
    load_prompts()

    logger.info("Qwen ëª¨ë¸ ë¡œë”© ì¤‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
    start_time = time.time()

    try:
        if USE_VLLM:
            client = OpenAI(base_url=VLLM_ENDPOINT, api_key="vllm")
            logger.info(f"vLLM í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ì„¤ì • ì™„ë£Œ: {VLLM_ENDPOINT}")
        else:
            # Tokenizer ë¡œë“œ
            tokenizer = AutoTokenizer.from_pretrained(
                MODEL_NAME,
                trust_remote_code=True,
                use_fast=False
            )

            # ëª¨ë¸ ë¡œë“œ
            model = AutoModelForCausalLM.from_pretrained(
                MODEL_NAME,
                torch_dtype=torch.float16,
                device_map="auto",
                trust_remote_code=True
            )

            # Pipeline ìƒì„±
            llm_pipe = pipeline(
                "text-generation",
                model=model,
                tokenizer=tokenizer
            )
            logger.info("transformers ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

        load_time = time.time() - start_time
        logger.info(f"ëª¨ë¸ ë¡œë”© ì™„ë£Œ! (ì†Œìš” ì‹œê°„: {load_time:.2f}ì´ˆ)")

        server_start_time = time.time()
        logger.info("ì„œë²„ ì¤€ë¹„ ì™„ë£Œ!")
        logger.info("=" * 60)

    except Exception as e:
        logger.error(f"ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        raise

    yield

    # ì¢…ë£Œ
    logger.info("ì„œë²„ ì¢…ë£Œ ì¤‘...")
    llm_pipe = None
    client = None
    server_start_time = 0
    tokenizer = None
    logger.info("ì„œë²„ ì¢…ë£Œ ì™„ë£Œ")

# ============================================================================
# FastAPI ì•± ìƒì„±
# ============================================================================

app = FastAPI(
    title="Qwen LLM API",
    description="Qwen ê¸°ë°˜ ì¸í…íŠ¸ ë¶„ë¥˜ ë° ì‘ë‹µ ìƒì„± API ì„œë²„",
    version="1.0.0",
    lifespan=lifespan,
    dependencies=[Depends(get_api_key)]  # â­ ëª¨ë“  ìš”ì²­ì— API Key ì¸ì¦ ì ìš©
)

# CORS ì„¤ì • (ë³´ì•ˆ ê°•í™”)
origins = [
    "http://localhost:8080",
    "http://127.0.0.1:8080",
    "https://recipeu.site"
]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ============================================================================
# API ì—”ë“œí¬ì¸íŠ¸
# ============================================================================

@app.get("/health", response_model=HealthResponse, tags=["ì‹œìŠ¤í…œ"])
async def health_check():
    """ì„œë²„ ìƒíƒœ í™•ì¸"""
    uptime = time.time() - server_start_time if server_start_time > 0 else 0

    # vLLM ë˜ëŠ” transformers ëª¨ë¸ ë¡œë“œ ì—¬ë¶€ í™•ì¸
    model_loaded = client is not None if USE_VLLM else llm_pipe is not None

    return HealthResponse(
        status="healthy" if model_loaded else "loading",
        model_loaded=model_loaded,
        model_name=MODEL_NAME,
        device=DEVICE,
        uptime_seconds=round(uptime, 2),
    )

@app.post("/classify", response_model=ClassifyResponse, tags=["ì¸í…íŠ¸"])
async def classify(request: ClassifyRequest):
    """
    ì¸í…íŠ¸ ë¶„ë¥˜ ë° ì‘ë‹µ ìƒì„±

    Parameters:
        text: ì‚¬ìš©ì ë°œí™”
        current_step: í˜„ì¬ ìš”ë¦¬ ë‹¨ê³„

    Returns:
        ClassifyResponse: ì¸í…íŠ¸ ë° ì‘ë‹µ
    """
    # ëª¨ë¸ ë¡œë“œ í™•ì¸
    if USE_VLLM:
        if not client:
            raise HTTPException(status_code=503, detail="SDK í´ë¼ì´ì–¸íŠ¸ ë¯¸ì„¤ì •")
    else:
        if not llm_pipe:
            raise HTTPException(status_code=503, detail="ëª¨ë¸ì´ ì•„ì§ ë¡œë”© ì¤‘ì…ë‹ˆë‹¤")

    logger.info(f"ë¶„ë¥˜ ìš”ì²­: '{request.text[:50]}...'")
    start_time = time.time()

    try:
        # ëŒ€í™” ê¸°ë¡ì„ í…ìŠ¤íŠ¸ë¡œ í¬ë§¤íŒ… (í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì˜ {chat_history}ì— ì‚½ì…)
        if request.history:
            chat_history_lines = []
            for h in request.history:
                role_label = "ì‚¬ìš©ì" if h.get("role") == "user" else "AI"
                chat_history_lines.append(f"- {role_label}: {h.get('content', '')}")
            chat_history_text = "\n".join(chat_history_lines)
        else:
            chat_history_text = "(ì´ì „ ëŒ€í™” ì—†ìŒ)"

        prompt = get_prompt(
            "unified_handler",
            text=request.text,
            current_step=request.current_step,
            current_cook=request.current_cook,
            recipe_context=request.recipe_context,
            chat_history=chat_history_text
        )

        # LLM í˜¸ì¶œ
        messages = [
            {"role": "system", "content": "ë„ˆëŠ” ì‚¬ìš©ìì˜ ìš”ë¦¬ ê³¼ì •ì„ ë•ëŠ” ë˜‘ë˜‘í•œ ì‰í”„ ì¡°ìˆ˜ì•¼."},
            {"role": "user", "content": prompt}
        ]

        if USE_VLLM:
            chat_response = client.chat.completions.create(
                model=MODEL_NAME,
                messages=messages,
                temperature=0.2,
                max_tokens=256
            )
            # ğŸ” ì—¬ê¸°! ë”± ì´ ì¤„ ì¶”ê°€
            # logger.info(f"message dump: {chat_response.choices[0].message}")

            msg = chat_response.choices[0].message
            raw_output = (msg.content or "").strip()

            if not raw_output:
                logger.warning("Empty content from vLLM")
                raw_output = "{}"
            
            # raw_output = chat_response.choices[0].message.content.strip()
        else:
            outputs = llm_pipe(
                messages,
                max_new_tokens=256,
                do_sample=True,
                temperature=0.5,
                pad_token_id=tokenizer.eos_token_id
            )

            # ê²°ê³¼ ì¶”ì¶œ
            raw_output = outputs[0]['generated_text'][-1]['content'].strip()

        logger.debug(f"LLM ì›ë³¸ ì¶œë ¥: {raw_output}")

        # JSON íŒŒì‹±
        data = extract_json(raw_output) or {}

        # -------- Intent ì¶”ì¶œ (None/í‚¤ë³€í˜• ë°©ì–´) --------
        raw_intent_val = (
            data.get("Intent")
            or data.get("intent")
            or data.get("INTENT")
            or "Out of Scope"
        )

        raw_intent = str(raw_intent_val).strip()
        intent = INTENT_MAP.get(raw_intent, "unknown")

        # -------- Response ì¶”ì¶œ (null -> "" ë°©ì–´) --------
        response_val = (
            data.get("Response")
            or data.get("response")
            or data.get("responseText")
            or data.get("ResponseText")
            or ""
        )

        response = str(response_val).strip()

        duration_ms = (time.time() - start_time) * 1000

        logger.info(f"ë¶„ë¥˜ ì™„ë£Œ: {intent} / '{response[:50]}...' ({duration_ms:.0f}ms)")

        return ClassifyResponse(
            success=True,
            intent=intent,
            response=response,
            raw_output=raw_output,
            duration_ms=round(duration_ms, 2)
        )

    except Exception as e:
        logger.error(f"ë¶„ë¥˜ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ë¶„ë¥˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

# ============================================================================
# ë©”ì¸ ì‹¤í–‰
# ============================================================================

if __name__ == "__main__":
    import uvicorn

    print("""
================================================================================
Qwen LLM API ì„œë²„

API ë¬¸ì„œ: http://localhost:8013/docs
ìƒíƒœ í™•ì¸: http://localhost:8013/health

ì£¼ì˜: venv_llm í™˜ê²½ì—ì„œ ì‹¤í–‰í•´ì•¼ í•©ë‹ˆë‹¤!
    source venv_llm/bin/activate
    python servers/llm_server.py
================================================================================
""")

    uvicorn.run(
        "llm_server:app",
        host="0.0.0.0",
        port=8013,
        reload=False,
        workers=1,
    )